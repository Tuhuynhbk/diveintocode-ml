{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Student Name: Huynh Truong Tu\n",
        " Below is my assignment for Sprint10's \"Deep Neural Scratch\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRSnDIbO6kLK"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EMnIuiAl6PFU"
      },
      "outputs": [],
      "source": [
        "#dependencies\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaiEVcL8hUPN"
      },
      "source": [
        "# Data Set Prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYvxe-vn6llv",
        "outputId": "ec57d7a8-64b1-4188-f254-cebf67938398"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ADMIN\\AppData\\Local\\Temp/ipykernel_12424/3284439299.py:8: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  X_train = X_train.astype(np.float)\n",
            "C:\\Users\\ADMIN\\AppData\\Local\\Temp/ipykernel_12424/3284439299.py:9: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  X_test = X_test.astype(np.float)\n"
          ]
        }
      ],
      "source": [
        "#data set\n",
        "from keras.datasets import mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "#reshape\n",
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)\n",
        "#scaling\n",
        "X_train = X_train.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "#one hot encode for multiclass labels!\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
        "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
        "#validation split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4X-mPbdhXXl"
      },
      "source": [
        "# Mini Batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1JbQAtYB66z-"
      },
      "outputs": [],
      "source": [
        "#mini batch\n",
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "Iterator to get a mini-batch\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : The following forms of ndarray, shape (n_samples, n_features)\n",
        "      Training data\n",
        "    y : The following form of ndarray, shape (n_samples, 1)\n",
        "      Correct answer value\n",
        "    batch_size : int\n",
        "      Batch size\n",
        "    seed : int\n",
        "      NumPy random number seed\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self._X = X[shuffle_index]\n",
        "        self._y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self._X[p0:p1], self._y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self._X[p0:p1], self._y[p0:p1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Tixzo3OhZ2j"
      },
      "source": [
        "# Old Prototype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gcF58I5t7dYT"
      },
      "outputs": [],
      "source": [
        "# 3 layers prototype\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class Scratch3LayerNeural():\n",
        "    def __init__(self, max_iter = 50, lr = 0.1, encoder = enc, verbose = False, debug = False):\n",
        "        self.epoch = max_iter\n",
        "        self.verbose = verbose\n",
        "        self.debug = debug\n",
        "        self.lr = lr\n",
        "        #other non-parametric vars:\n",
        "        self.encoder = encoder\n",
        "        self.activation = sigmoid\n",
        "        self.sigma = 0.01\n",
        "        self.batch_size = 20 # batch size \n",
        "        self.n_features = 784 # number of features \n",
        "        self.n_nodes1 = 400 # number of first layer nodes \n",
        "        self.n_nodes2 = 200 # number of second layer nodes \n",
        "        self.n_output = 10 # number of output classes (number of nodes in the 3rd layer)\n",
        "\n",
        "    def _init_params_and_output_container(self, prev_n_nodes, current_n_nodes):\n",
        "        weight = self.sigma * np.random.randn(prev_n_nodes,current_n_nodes)\n",
        "        bias = self.sigma * np.random.randn(1,current_n_nodes)\n",
        "        return weight,bias\n",
        "    def fit(self,X,y, X_val = None, y_val = None):\n",
        "        #prepare\n",
        "        self.n_features = X.shape[1]\n",
        "        self.lenx = len(X)\n",
        "        self.batch_count = len(GetMiniBatch(X,y,batch_size= self.batch_size)) #for debug\n",
        "        #init weights,bias and z_container\n",
        "        self.W1,self.B1 = self._init_params_and_output_container(n_features,self.n_nodes1)\n",
        "        self.W2,self.B2 = self._init_params_and_output_container(self.n_nodes1,self.n_nodes2)\n",
        "        self.W3,self.B3 = self._init_params_and_output_container(self.n_nodes2,self.n_output)\n",
        "        if self.verbose:\n",
        "            print('X shape: ', X.shape, 'type: ', X.dtype)\n",
        "            print('Batch count: ', self.batch_count)\n",
        "            print('1st layer: ', self.W1.shape, self.B1.shape)\n",
        "            print('2nd layer: ', self.W2.shape, self.B2.shape)\n",
        "            print('3rd layer: ', self.W3.shape, self.B3.shape)\n",
        "\n",
        "        #train\n",
        "        self.loss = np.zeros(self.epoch)\n",
        "        self.accuracy = np.zeros(self.epoch)\n",
        "        for i in range(self.epoch): #one full data ilteration\n",
        "            if self.verbose: print('Epoch: ', i)\n",
        "            self.get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\n",
        "            for idx, (mini_X_train, mini_y_train) in enumerate(self.get_mini_batch):\n",
        "                if self.debug: print('Current batch: ', idx)\n",
        "                #train mini_batch\n",
        "                self.forward_prop(mini_X_train,mini_y_train)\n",
        "                self.backward_prop(mini_X_train,mini_y_train)\n",
        "\n",
        "\n",
        "            #record loss data\n",
        "            Z3 = self.forward_prop(X,y,update = False)\n",
        "            self.loss[i] = self.cross_entropy_error(Z3,y)\n",
        "            train_pred = self.predict(X)\n",
        "            self.accuracy[i]  = accuracy_score(train_pred,y)\n",
        "            if self.verbose:\n",
        "                print(f'Loss {i}:', self.loss[i])\n",
        "                print(f'Acc {i}:', self.accuracy[i])\n",
        "                \n",
        "        #verbose\n",
        "        if self.verbose:\n",
        "            print('Final train loss:',self.loss[-1])\n",
        "            print('Final train accuracy:',self.accuracy[-1])\n",
        "\n",
        "    def forward_prop(self,X,y, update = True):\n",
        "        activation = self.activation\n",
        "        W1,B1 = self.W1,self.B1\n",
        "        W2,B2 = self.W2,self.B2\n",
        "        W3,B3 = self.W3,self.B3\n",
        "        # first layer\n",
        "        A1 = X@W1 + B1\n",
        "        Z1= activation(A1)\n",
        "        # second layer\n",
        "        A2 = Z1@W2 + B2\n",
        "        Z2 = activation(A2)\n",
        "        #last (third) layer\n",
        "        A3 = Z2@W3 + B3\n",
        "        Z3 = softmax(A3)\n",
        "\n",
        "        if update: #for training\n",
        "            self.A1 = A1\n",
        "            self.Z1 = Z1\n",
        "            self.A2 = A2\n",
        "            self.Z2 = Z2\n",
        "            self.A3 = A3\n",
        "            self.Z3 = Z3\n",
        "        else: # for predicting\n",
        "            return Z3\n",
        "\n",
        "    def backward_prop(self,X,y):\n",
        "        A1 = self.A1 \n",
        "        Z1 = self.Z1 \n",
        "        A2 = self.A2 \n",
        "        Z2 = self.Z2 \n",
        "        A3 = self.A3 \n",
        "        Z3 = self.Z3 \n",
        "        #third\n",
        "        grad_L_A3 = 1/nb * (Z3 - y)\n",
        "        grad_L_B3 = grad_L_A3.sum(axis = 0).reshape(1,-1)\n",
        "        grad_L_W3 = Z2.T @ grad_L_A3\n",
        "        grad_L_Z2 = grad_L_A3 @ W3.T\n",
        "        #second\n",
        "        grad_L_A2 = grad_L_Z2 * (1 - np.tanh(A2)** 2)\n",
        "        grad_L_B2 = grad_L_A2.sum(axis = 0).reshape(1,-1)\n",
        "        grad_L_W2 = Z1.T @ grad_L_A2\n",
        "        grad_L_Z1 = grad_L_A2 @ W2.T\n",
        "        #first\n",
        "        grad_L_A1 = grad_L_Z1 * (1 - np.tanh(A1) ** 2)\n",
        "        grad_L_B1 = grad_L_A1.sum(axis = 0).reshape(1,-1)\n",
        "        grad_L_W1 = X.T @ grad_L_A1\n",
        "        \n",
        "        #new params\n",
        "        lr = self.lr\n",
        "        self.W1 += - lr * grad_L_W1\n",
        "        self.B1 += - lr * grad_L_B1\n",
        "        self.W2 += - lr * grad_L_W2\n",
        "        self.B2 += - lr * grad_L_B2\n",
        "        self.W3 += - lr * grad_L_W3\n",
        "        self.B3 += - lr * grad_L_B3\n",
        "        \n",
        "    \n",
        "    def cross_entropy_error(self,Z3,y):\n",
        "        return (np.log(Z3) * y).sum() / (- len(Z3))\n",
        "\n",
        "    def predict(self,X):\n",
        "        y = np.zeros(X.shape[0])\n",
        "        Z3  = self.forward_prop(X,y,update = False)\n",
        "        return self.encoder.transform(np.argmax(Z3, axis = 1).reshape(-1,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R81UNdtEhceP"
      },
      "source": [
        "# Problem 1 - Classifying fully connected layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "y6H97BS0-ZeL"
      },
      "outputs": [],
      "source": [
        "class FC:\n",
        "  \"\"\"\n",
        "  Number of nodes Fully connected layer from n_nodes1 to n_nodes2\n",
        "  Parameters\n",
        "  ----------\n",
        "  n_nodes1 : int\n",
        "    Number of nodes in the previous layer\n",
        "  n_nodes2 : int\n",
        "    Number of nodes in the later layer\n",
        "  initializer: instance of initialization method\n",
        "  optimizer: instance of optimization method\n",
        "  \"\"\"\n",
        "  def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "      # Initialize\n",
        "      self.optimizer = optimizer\n",
        "      self.n_nodes1, self.n_nodes2 = n_nodes1, n_nodes2\n",
        "      # Initialize self.W and self.B using the initializer method\n",
        "      self.W = initializer.W(n_nodes1, n_nodes2)\n",
        "      self.B = initializer.B(n_nodes2)\n",
        "      pass\n",
        "  def forward(self, X):\n",
        "      \"\"\"\n",
        "      forward\n",
        "      Parameters\n",
        "      ----------\n",
        "      X : The following forms of ndarray, shape (batch_size, n_nodes1)\n",
        "          入力\n",
        "      Returns\n",
        "      ----------\n",
        "      A : The following forms of ndarray, shape (batch_size, n_nodes2)\n",
        "          output\n",
        "      \"\"\"        \n",
        "      self.X = X\n",
        "      A = X @ self.W + self.B\n",
        "      return A\n",
        "  def backward(self, dA):\n",
        "      \"\"\"\n",
        "      Backward\n",
        "      Parameters\n",
        "      ----------\n",
        "      dA : The following forms of ndarray, shape (batch_size, n_nodes2)\n",
        "          Gradient flowing from behind\n",
        "      Returns\n",
        "      ----------\n",
        "      dZ : The following forms of ndarray, shape (batch_size, n_nodes1)\n",
        "          Gradient to flow forward\n",
        "      \"\"\"\n",
        "      # update\n",
        "      self.dA = dA\n",
        "\n",
        "      self.dZ = None #update in optimizer!\n",
        "      self = self.optimizer.update(self)\n",
        "      return self.dZ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deyYXwH9hjeb"
      },
      "source": [
        "# Problem 2 - Classifying initialization method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-AkIjsn3Aqlp"
      },
      "outputs": [],
      "source": [
        "class SimpleInitializer:\n",
        "  \"\"\"\n",
        "  Simple initialization with Gaussian distribution\n",
        "  Parameters\n",
        "  ----------\n",
        "  sigma : float\n",
        "    Standard deviation of Gaussian distribution\n",
        "  \"\"\"\n",
        "  def __init__(self, sigma):\n",
        "    self.sigma = sigma\n",
        "  def W(self, n_nodes1, n_nodes2):\n",
        "    \"\"\"\n",
        "    Weight initialization\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_nodes1 : int\n",
        "      Number of nodes in the previous layer\n",
        "    n_nodes2 : int\n",
        "      Number of nodes in the later layer\n",
        "    Returns\n",
        "    ----------\n",
        "    W :\n",
        "    \"\"\"\n",
        "    W = self.sigma * np.random.randn(n_nodes1,n_nodes2)\n",
        "    return W\n",
        "  def B(self, n_nodes2):\n",
        "    \"\"\"\n",
        "    Bias initialization\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_nodes2 : int\n",
        "      Number of nodes in the later layer\n",
        "    Returns\n",
        "    ----------\n",
        "    B :\n",
        "    \"\"\"\n",
        "    B = self.sigma * np.random.randn(1,n_nodes2)\n",
        "    return B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT9yX5UrhnQ7"
      },
      "source": [
        "# Problem 3 - Classifying Optimization method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CvjcuJMkB1_y"
      },
      "outputs": [],
      "source": [
        "class SGD:\n",
        "    \"\"\"\n",
        "    Stochastic gradient descent\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : Learning rate\n",
        "    \"\"\"\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        Update weights and biases for a layer\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : Instance of the layer before update\n",
        "        \"\"\"\n",
        "        dA = layer.dA #this is flow from back\n",
        "        X = layer.X # input to the layer (X or Z)\n",
        "        W,B = layer.W, layer.B\n",
        "\n",
        "        dB = dA.sum(axis = 0).reshape(1,-1)  \n",
        "        dW = X.T @ dA\n",
        "        dZ = dA @ W.T #this will flow to the front\n",
        "        #update\n",
        "        layer.B += - self.lr * dB\n",
        "        layer.W += - self.lr * dW\n",
        "        layer.dZ = dZ\n",
        "        return layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RrMxj2BhsKM"
      },
      "source": [
        "# Problem 4 - Classifying Activation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "912e8_nAIFRC"
      },
      "outputs": [],
      "source": [
        "# Problem 4: Classifying activation function\n",
        "class ActivationFunction():\n",
        "  def forward(self,A):\n",
        "    pass\n",
        "  def backward(self,dZ):\n",
        "    pass\n",
        "class Sigmoid(ActivationFunction):\n",
        "  def func(self,A):\n",
        "    return 1/(1+np.exp(-A))\n",
        "  def forward(self,A):\n",
        "    self.A = A\n",
        "    return self.func(A)\n",
        "  def backward(self,dZ):\n",
        "    A = self.A\n",
        "    dA = dZ * (1 - self.func(A))@self.func(A)\n",
        "class Tanh(ActivationFunction):\n",
        "  def forward(self,A):\n",
        "    self.A = A\n",
        "    Z = (np.exp(A) - np.exp(-A)) / (np.exp(A) + np.exp(-A))\n",
        "    return Z\n",
        "  def backward(self,dZ):\n",
        "    A = self.A\n",
        "    dA = dZ * (1 - np.tanh(A) ** 2)\n",
        "    return dA\n",
        "  \n",
        "class SoftMax(ActivationFunction):\n",
        "  def forward(self,A):\n",
        "    self.A = A\n",
        "    Z = np.exp(A) / np.sum(np.exp(A), axis = 1).reshape(-1,1)\n",
        "    return Z\n",
        "  def backward(self,Z,Y):\n",
        "    A = self.A\n",
        "    nb = Z.shape[0]\n",
        "    dA = 1/nb * (Z - Y)\n",
        "    return dA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpQth5ppif8b"
      },
      "source": [
        "# Deep Neural Network Prototype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aR-7rucDPTQU"
      },
      "outputs": [],
      "source": [
        "# the deepneural net proto\n",
        "class ScratchDeepNeuralNetworkClassifier():\n",
        "    def __init__(self, max_iter = 50, lr = 0.1, encoder = enc, verbose = False, debug = False):\n",
        "        self.epoch = max_iter\n",
        "        self.verbose = verbose\n",
        "        self.debug = debug\n",
        "        self.lr = lr\n",
        "        #other non-parametric vars:\n",
        "        self.encoder = encoder\n",
        "        self.sigma = 0.01\n",
        "        self.batch_size = 20 # batch size \n",
        "        self.n_features = 784 # number of features \n",
        "        self.n_nodes1 = 400 # number of first layer nodes \n",
        "        self.n_nodes2 = 200 # number of second layer nodes \n",
        "        self.n_output = 10 # number of output classes (number of nodes in the 3rd layer)\n",
        "        #layers\n",
        "        optimizer = SGD(self.lr)\n",
        "        self.FC1 = FC(self.n_features, self.n_nodes1, SimpleInitializer(self.sigma), optimizer)\n",
        "        self.activation1 = Tanh()\n",
        "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, SimpleInitializer(self.sigma), optimizer)\n",
        "        self.activation2 = Tanh()\n",
        "        self.FC3 = FC(self.n_nodes2, self.n_output, SimpleInitializer(self.sigma), optimizer)\n",
        "        self.activation3 = SoftMax()\n",
        "        #wrap up\n",
        "        self.layers = [self.FC1,self.FC2, self.FC3]\n",
        "        self.activations = [self.activation1, self.activation2, self.activation3]\n",
        "    def enum_layer_act(self, rev = False):\n",
        "        zipped = zip(self.layers, self.activations)\n",
        "        if rev:\n",
        "          return enumerate(reversed(list(zipped)))\n",
        "        return enumerate(zipped)\n",
        "    def fit(self,X,y, X_val = None, y_val = None):\n",
        "        #prepare\n",
        "        self.n_features = X.shape[1]\n",
        "        self.lenx = len(X)\n",
        "        self.batch_count = len(GetMiniBatch(X,y,batch_size= self.batch_size)) #for debug\n",
        "\n",
        "        if self.verbose:\n",
        "            print('X shape: ', X.shape, 'type: ', X.dtype)\n",
        "            print('Batch count: ', self.batch_count)\n",
        "            for i, (layer, activation) in self.enum_layer_act():\n",
        "              print(f'Layer {i+1}: ', layer.n_nodes1, layer.n_nodes2)\n",
        "              print(f'Activ: {i+1}:', activation.__class__.__name__)\n",
        "\n",
        "        #train\n",
        "        self.loss = np.zeros(self.epoch)\n",
        "        self.accuracy = np.zeros(self.epoch)\n",
        "        for i in range(self.epoch): #one full data ilteration\n",
        "            if self.verbose: print('Epoch: ', i)\n",
        "            self.get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\n",
        "            for idx, (mini_X_train, mini_y_train) in enumerate(self.get_mini_batch):\n",
        "                if self.debug: print('Current batch: ', idx)\n",
        "                #train mini_batch\n",
        "                Z = self.forward_prop(mini_X_train)\n",
        "                self.backward_prop(Z,mini_y_train)\n",
        "\n",
        "\n",
        "            #record loss data\n",
        "            Z = self.forward_prop(X)\n",
        "            self.loss[i] = self.cross_entropy_error(Z,y)\n",
        "            train_pred = self.predict(X)\n",
        "            self.accuracy[i]  = accuracy_score(train_pred,y)\n",
        "            if self.verbose:\n",
        "                print(f'Loss {i}:', self.loss[i])\n",
        "                print(f'Acc {i}:', self.accuracy[i])\n",
        "                \n",
        "        #verbose\n",
        "        if self.verbose:\n",
        "            print('Final train loss:',self.loss[-1])\n",
        "            print('Final train accuracy:',self.accuracy[-1])\n",
        "\n",
        "    def forward_prop(self,X):\n",
        "        Z = X\n",
        "        for i, (layer, activation) in self.enum_layer_act():\n",
        "          A = layer.forward(Z)\n",
        "          Z = activation.forward(A)\n",
        "          if self.debug:\n",
        "            print(f'Z{i+1}: ', Z.shape, A.shape)\n",
        "        return Z\n",
        "\n",
        "    def backward_prop(self,Z,y):\n",
        "        dA = self.activations[-1].backward(Z,y)\n",
        "        for i, (layer, activation) in self.enum_layer_act(rev = True):\n",
        "          if i == 0: #last layer has different activation backward!\n",
        "            dZ = layer.backward(dA)\n",
        "            continue\n",
        "          dA = activation.backward(dZ)\n",
        "          dZ = layer.backward(dA)\n",
        "        \n",
        "    \n",
        "    def cross_entropy_error(self,Z,y):\n",
        "        return (np.log(Z) * y).sum() / (- len(Z))\n",
        "\n",
        "    def predict(self,X):\n",
        "        y = np.zeros(X.shape[0])\n",
        "        Z  = self.forward_prop(X)\n",
        "        return self.encoder.transform(np.argmax(Z, axis = 1).reshape(-1,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yamoSBB9YIdy",
        "outputId": "60a63480-6031-4acd-d4bb-f2e43d85379c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ADMIN\\AppData\\Local\\Temp/ipykernel_12424/1544501054.py:22: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X shape:  (48000, 784) type:  float64\n",
            "Batch count:  2400\n",
            "Layer 1:  784 400\n",
            "Activ: 1: Tanh\n",
            "Layer 2:  400 200\n",
            "Activ: 2: Tanh\n",
            "Layer 3:  200 10\n",
            "Activ: 3: SoftMax\n",
            "Epoch:  0\n",
            "Loss 0: 0.2544986324627638\n",
            "Acc 0: 0.9238958333333334\n",
            "Epoch:  1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ADMIN\\AppData\\Local\\Temp/ipykernel_12424/1544501054.py:22: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss 1: 0.16230532039331802\n",
            "Acc 1: 0.9506458333333333\n",
            "Epoch:  2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ADMIN\\AppData\\Local\\Temp/ipykernel_12424/1544501054.py:22: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss 2: 0.11880331938520491\n",
            "Acc 2: 0.9631875\n",
            "Epoch:  3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ADMIN\\AppData\\Local\\Temp/ipykernel_12424/1544501054.py:22: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss 3: 0.0899072372201784\n",
            "Acc 3: 0.9720833333333333\n",
            "Epoch:  4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ADMIN\\AppData\\Local\\Temp/ipykernel_12424/1544501054.py:22: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss 4: 0.06978653952804144\n",
            "Acc 4: 0.9777291666666666\n",
            "Final train loss: 0.06978653952804144\n",
            "Final train accuracy: 0.9777291666666666\n"
          ]
        }
      ],
      "source": [
        "model = ScratchDeepNeuralNetworkClassifier(max_iter = 5,verbose = True, debug = False)\n",
        "model.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6K0XA7bilU3"
      },
      "source": [
        "# Problem 5 - ReLU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "18EJf11ilWXK"
      },
      "outputs": [],
      "source": [
        "class ReLU(ActivationFunction):\n",
        "  def forward(self,A):\n",
        "    self.A = A\n",
        "    relu = A * np.where(A > 0, 1,0)\n",
        "    return relu\n",
        "  def backward(self,dZ):\n",
        "    df = np.where(self.A > 0, 1,0)\n",
        "    dA = dZ * df\n",
        "    return dA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuAbTweyivWD"
      },
      "source": [
        "# Problem 6 - Init value of Weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Yxpfh6sqn5zI"
      },
      "outputs": [],
      "source": [
        "#sample\n",
        "# W = self.sigma * np.random.randn(n_nodes1,n_nodes2)\n",
        "# B = self.sigma * np.random.randn(1,n_nodes2)\n",
        "#..\n",
        "\n",
        "class XavierInitializer():\n",
        "  def __init__(self,sigma):\n",
        "    pass\n",
        "  def W(self, n_nodes1, n_nodes2):\n",
        "    self.sigma = 1 / np.sqrt(n_nodes1)\n",
        "    W = self.sigma * np.random.randn(n_nodes1,n_nodes2)\n",
        "    return W\n",
        "  def B(self, n_nodes2):\n",
        "    B = self.sigma * np.random.randn(1,n_nodes2)\n",
        "    return B\n",
        "\n",
        "class HelnInitializer():\n",
        "  def __init__(self,sigma):\n",
        "    pass\n",
        "  def W(self, n_nodes1, n_nodes2):\n",
        "    self.sigma = np.sqrt(2/n_nodes1)\n",
        "    W = self.sigma * np.random.randn(n_nodes1,n_nodes2)\n",
        "    return W\n",
        "  def B(self, n_nodes2):\n",
        "    B = self.sigma * np.random.randn(1,n_nodes2)\n",
        "    return B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMrx2c43u0fg"
      },
      "source": [
        "# Problem 7 - Optimization Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "izHlJYC1qagp"
      },
      "outputs": [],
      "source": [
        "class AdaGrad():\n",
        "    \"\"\"\n",
        "    Stochastic gradient descent\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : Learning rate\n",
        "    \"\"\"\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "        self.Hw = 0\n",
        "        self.Hb = 0\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        Update weights and biases for a layer\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : Instance of the layer before update\n",
        "        \"\"\"\n",
        "        dA = layer.dA #this is flow from back\n",
        "        X = layer.X # input to the layer (X or Z)\n",
        "        W,B = layer.W, layer.B\n",
        "\n",
        "        dB = dA.sum(axis = 0).reshape(1,-1)  \n",
        "        dW = X.T @ dA\n",
        "        dZ = dA @ W.T #this will flow to the front\n",
        "        #ada weight\n",
        "        eps = 1e-6\n",
        "\n",
        "        self.Hw += dW ** 2\n",
        "        self.Hb += dB ** 2\n",
        "        \n",
        "        layer.W += - self.lr * (1 / np.sqrt(self.Hw + eps)) * dW\n",
        "        layer.B += - self.lr * (1 / np.sqrt(self.Hb + eps)) * dB\n",
        "        layer.dZ = dZ\n",
        "        return layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcZ83E8Fi6nu"
      },
      "source": [
        "# Problem 8 - Class Completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LI3x2GS2rhrY"
      },
      "outputs": [],
      "source": [
        "# Just fixing up the class to be flexible!\n",
        "class FinishedDeepNeuralNetworkClassifier(ScratchDeepNeuralNetworkClassifier):\n",
        "  def __init__(self, max_iter = 50, lr = 0.1, \n",
        "               layers_n_nodes = [400,200,10],\n",
        "               encoder = enc, optimizer_class = SGD,\n",
        "               activation_class = Tanh, initializer_class = SimpleInitializer,\n",
        "               verbose = False, debug = False):\n",
        "        self.epoch = max_iter\n",
        "        self.verbose = verbose\n",
        "        self.debug = debug\n",
        "        self.lr = lr\n",
        "        #other non-parametric vars:\n",
        "        self.encoder = encoder\n",
        "        self.sigma = 0.01\n",
        "        self.batch_size = 20 # batch size \n",
        " \n",
        "        #prep layers\n",
        "        self.layers_n_nodes = layers_n_nodes\n",
        "        self.initializer_class = initializer_class\n",
        "        self.activation_class = activation_class\n",
        "        self.optimizer_class = optimizer_class\n",
        "        self.layers = []\n",
        "        self.activations = []\n",
        "        for i in range(len(layers_n_nodes)): \n",
        "          if i == 0: continue #specify first layer later when have X\n",
        "          n_nodes1 = layers_n_nodes[i-1]\n",
        "          n_nodes2 = layers_n_nodes[i]\n",
        "          layer = FC(n_nodes1, n_nodes2, initializer_class(self.sigma), optimizer_class(self.lr))\n",
        "          self.layers.append(layer)\n",
        "          if i != len(layers_n_nodes)- 1: #last activation is softmax!\n",
        "            self.activations.append(activation_class())\n",
        "          else:\n",
        "            self.activations.append(SoftMax())\n",
        "  def fit(self,X,y, X_val = None, y_val = None):\n",
        "        first_layer = FC(X.shape[1], self.layers_n_nodes[0], self.initializer_class(self.sigma), self.optimizer_class(self.lr))\n",
        "        self.layers.insert(0,first_layer)\n",
        "        self.activations.insert(0,self.activation_class())\n",
        "        super(FinishedDeepNeuralNetworkClassifier,self).fit(X,y,X_val,y_val)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZQRNtw_05Mj"
      },
      "source": [
        "# Problem 9 - Learning and completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAm9V2Fk06mq",
        "outputId": "21da85ed-f547-4c1b-8098-d83fa37f4af5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ADMIN\\AppData\\Local\\Temp/ipykernel_12424/1544501054.py:22: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X shape:  (48000, 784) type:  float64\n",
            "Batch count:  2400\n",
            "Layer 1:  784 100\n",
            "Activ: 1: ReLU\n",
            "Layer 2:  100 50\n",
            "Activ: 2: ReLU\n",
            "Layer 3:  50 120\n",
            "Activ: 3: ReLU\n",
            "Layer 4:  120 10\n",
            "Activ: 4: SoftMax\n",
            "Epoch:  0\n",
            "Loss 0: 0.21813837393613256\n",
            "Acc 0: 0.9328958333333334\n",
            "Epoch:  1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ADMIN\\AppData\\Local\\Temp/ipykernel_12424/1544501054.py:22: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss 1: 0.15185767598184094\n",
            "Acc 1: 0.9548333333333333\n",
            "Epoch:  2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ADMIN\\AppData\\Local\\Temp/ipykernel_12424/1544501054.py:22: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss 2: 0.12718656761458033\n",
            "Acc 2: 0.9611666666666666\n",
            "Epoch:  3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ADMIN\\AppData\\Local\\Temp/ipykernel_12424/1544501054.py:22: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss 3: 0.11312254170293916\n",
            "Acc 3: 0.964625\n",
            "Epoch:  4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ADMIN\\AppData\\Local\\Temp/ipykernel_12424/1544501054.py:22: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss 4: 0.09814826818497913\n",
            "Acc 4: 0.9693958333333333\n",
            "Final train loss: 0.09814826818497913\n",
            "Final train accuracy: 0.9693958333333333\n"
          ]
        }
      ],
      "source": [
        "model = FinishedDeepNeuralNetworkClassifier(\n",
        "    max_iter = 5, lr = 0.1,\n",
        "    layers_n_nodes = [100,50,120,10],\n",
        "    optimizer_class = AdaGrad, \n",
        "    activation_class = ReLU,\n",
        "    initializer_class = XavierInitializer,\n",
        "    verbose = True, debug = False\n",
        "    \n",
        ")\n",
        "model.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "EEFCfS_njPUr",
        "outputId": "e8fdfc36-676f-4f5a-b891-a3c79113ee5a"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# loss result\n",
        "plt.plot(model.loss, label = 'TrainLoss')\n",
        "plt.plot(model.accuracy, label = 'TrainAccuracy')\n",
        "plt.title('Training Metrics')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is the end of my assignment. Thank you for reading!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMZ1QZeonUkYaL8ERfbLAj/",
      "include_colab_link": true,
      "name": "ScratchDeepNeural.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
