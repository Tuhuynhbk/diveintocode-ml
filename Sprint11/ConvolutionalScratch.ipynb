{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Student Name: Huynh Truong Tu\n",
                " Below is my assignment for Sprint11's \"Convolutional Scratch\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "-----------------------------------------------------------------------------------------"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Introduction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "#needed\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Needed Old Code"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "#intializer\n",
                "class SimpleInitializer:\n",
                "  def __init__(self, sigma = 0.1):\n",
                "    self.sigma = sigma\n",
                "  def W(self, n_nodes1, n_nodes2):\n",
                "    W = self.sigma * np.random.randn(n_nodes1,n_nodes2)\n",
                "    return W\n",
                "  def B(self, n_nodes2):\n",
                "\n",
                "    B = self.sigma * np.random.randn(1,n_nodes2)\n",
                "    return B\n",
                "class SGD:\n",
                "    \"\"\"\n",
                "    Stochastic gradient descent\n",
                "    Parameters\n",
                "    ----------\n",
                "    lr : Learning rate\n",
                "    \"\"\"\n",
                "    def __init__(self, lr=0.01):\n",
                "        self.lr = lr\n",
                "    def update(self, layer):\n",
                "        \"\"\"\n",
                "        Update weights and biases for a layer\n",
                "        Parameters\n",
                "        ----------\n",
                "        layer : Instance of the layer before update\n",
                "        \"\"\"\n",
                "        #update\n",
                "        layer.B += - self.lr * layer.dB\n",
                "        layer.W += - self.lr * layer.dW\n",
                "        return layer\n",
                "#! mini batch...........................\n",
                "class GetMiniBatch:\n",
                "    \"\"\"\n",
                "Iterator to get a mini-batch\n",
                "    Parameters\n",
                "    ----------\n",
                "    X : The following forms of ndarray, shape (n_samples, n_features)\n",
                "      Training data\n",
                "    y : The following form of ndarray, shape (n_samples, 1)\n",
                "      Correct answer value\n",
                "    batch_size : int\n",
                "      Batch size\n",
                "    seed : int\n",
                "      NumPy random number seed\n",
                "    \"\"\"\n",
                "    def __init__(self, X, y = None, batch_size = 20, seed=0):\n",
                "        self.batch_size = batch_size\n",
                "        np.random.seed(seed)\n",
                "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
                "        self._X = X[shuffle_index]\n",
                "        self._y = y[shuffle_index] if y is not None else None\n",
                "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
                "    def __len__(self):\n",
                "        return self._stop\n",
                "    def __getitem__(self,item):\n",
                "        p0 = item*self.batch_size\n",
                "        p1 = item*self.batch_size + self.batch_size\n",
                "        if self._y is not None:\n",
                "          return self._X[p0:p1], self._y[p0:p1] \n",
                "        else:\n",
                "          return self._X[p0:p1]       \n",
                "    def __iter__(self):\n",
                "        self._counter = 0\n",
                "        return self\n",
                "    def __next__(self):\n",
                "        if self._counter >= self._stop:\n",
                "            raise StopIteration()\n",
                "        p0 = self._counter*self.batch_size\n",
                "        p1 = self._counter*self.batch_size + self.batch_size\n",
                "        self._counter += 1\n",
                "        if self._y is not None:\n",
                "          return self._X[p0:p1], self._y[p0:p1] \n",
                "        else:\n",
                "          return self._X[p0:p1]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Problem 1 - 1 Dim convolutional Layer that limit number of channels to 1"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Forward"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SimpleConv1D:\n",
                "  def __init__(self, filter_size, initializer, optimizer):\n",
                "    # Initialize\n",
                "    self.optimizer = optimizer\n",
                "    self.filter_size = filter_size\n",
                "    # Initialize self.W and self.B using the initializer method\n",
                "    self.W = initializer.W(filter_size, 1)\n",
                "    self.B = initializer.B(1)\n",
                "  def forward(self, X):\n",
                "    filter_size = self.filter_size\n",
                "    self.X = X\n",
                "    return self.convolve(self.W,X, bias = self.B)\n",
                "\n",
                "  def backward(self, dA):\n",
                "    # update\n",
                "    self.dA = dA\n",
                "    self.dW = self.calc_dW()\n",
                "    self.dB = self.calc_dB()\n",
                "    self.dX = self.calc_dX()\n",
                "    self = self.optimizer.update(self)\n",
                "    return self.dW, self.dB, self.dX\n",
                "  def calc_dW(self):\n",
                "    return self.convolve(self.dA, self.X)\n",
                "  def calc_dB(self):\n",
                "    return np.array(self.dA.sum()).ravel()\n",
                "  def calc_dX(self):\n",
                "    padA = self.padA()\n",
                "    return self.convolve(np.flip(self.W),padA)\n",
                "  def padA(self):\n",
                "    padding = (len(self.X) - 1 + len(self.W) - len(self.dA)) //2 # cause out = (in - fil + 2pad) + 1\n",
                "    print('Padding size: ', padding)\n",
                "    return np.pad(self.dA,(padding, padding), mode = 'constant')\n",
                "  def convolve(self,F,X,bias = [0]):\n",
                "    A = []\n",
                "    filter_size = len(F)\n",
                "    for i in range(len(X) - filter_size + 1):\n",
                "      A.append(X[i : i + filter_size] @ F + bias[0])\n",
                "    return np.array(A)\n",
                "\n",
                "    \n",
                "  "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Problem 2 - Output size calculation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Convolutional n-out of input 30 and filter 3: 28\n"
                    ]
                }
            ],
            "source": [
                "def conv_n_out(n_in,filter_size, padding=0, stride=1):\n",
                "    return (n_in + 2*padding - filter_size) //stride + 1\n",
                "print('Convolutional n-out of input 30 and filter 3:', conv_n_out(30,3))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Problem 3 - Experiment of one-dimensional convolutional layer with small array"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "n_output:  2\n",
                        "forward:  [35. 50.] correct result:  [35 50]\n",
                        "Padding size:  2\n",
                        "dB:  [30] correct result:  [30]\n",
                        "dW:  [ 50  80 110] correct result:  [ 50  80 110]\n",
                        "dX:  [ 30. 110. 170. 140.] correct result:  [ 30 110 170 140]\n"
                    ]
                }
            ],
            "source": [
                "x = np.array([1,2,3,4])\n",
                "w = np.array([3, 5, 7], dtype = np.float64)\n",
                "b = np.array([1], dtype = np.float64)\n",
                "layer = SimpleConv1D(3,SimpleInitializer(),SGD())\n",
                "layer.W = w\n",
                "layer.B = b\n",
                "\n",
                "print('n_output: ', conv_n_out(len(x),len(w)))\n",
                "\n",
                "correct_forward_result = np.array([35, 50])\n",
                "a = layer.forward(x)\n",
                "print('forward: ', layer.forward(x), 'correct result: ', correct_forward_result)\n",
                "\n",
                "\n",
                "delta_a = np.array([10, 20])\n",
                "dW,dB,dX = layer.backward(delta_a)\n",
                "\n",
                "\n",
                "correct_delta_b = np.array([30])\n",
                "correct_delta_x = np.array([30, 110, 170, 140])\n",
                "correct_delta_w = np.array([50, 80, 110])\n",
                "print('dB: ', dB, 'correct result: ', correct_delta_b)\n",
                "print('dW: ', dW,'correct result: ', correct_delta_w)\n",
                "print('dX: ', dX, 'correct result: ', correct_delta_x)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Problem 4 - Creating a one-dimensional convolutional layer class that does not limit the number of channels"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Updated Prototype (NO STRIDE, NO BATCH)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Conv1D:\n",
                "  def __init__(self, filter_size = 3, n_input = 2, n_output = 3, optimizer = SGD(), padding = 0):\n",
                "    # Initialize\n",
                "    self.optimizer = optimizer\n",
                "    self.filter_size = filter_size\n",
                "    # Initialize self.W and self.B using the initializer method\n",
                "    self.filter_size = filter_size\n",
                "    self.n_input = n_input\n",
                "    self.n_output = n_output\n",
                "\n",
                "    self.W = np.ones((n_output, n_input,filter_size))\n",
                "    self.B = np.ones(n_output)\n",
                "    #padding and stride\n",
                "    self.padding = padding\n",
                "  def forward(self, X): \n",
                "    X = np.pad(X,[(0,0),(self.padding, self.padding)], mode = 'constant')\n",
                "    self.X = X\n",
                "    output = []\n",
                "    for i in range(self.n_output):\n",
                "      filt = self.W[i]\n",
                "      bias = self.B[i]\n",
                "      conv = self.convolve(filt, X)\n",
                "      output.append(conv.sum(axis = 0) + bias)\n",
                "    return np.array(output)\n",
                "  def backward(self, dA):\n",
                "    # update\n",
                "    self.dA = dA\n",
                "    self.dW = self.calc_dW()\n",
                "    self.dB = self.calc_dB()\n",
                "    self.dX = self.calc_dX()\n",
                "    self = self.optimizer.update(self)\n",
                "    # print(f'After backward: x {self.X.shape}, w: {self.W.shape}, b: {self.B.shape}, dX: {self.dX.shape}, dW: {self.dW.shape}, dB: {self.dB.shape}, dA: {self.dA.shape}')\n",
                "    # print(f'filterz: {self.filter_size}')\n",
                "    return self.dW, self.dB, self.dX\n",
                "  def calc_dW(self):\n",
                "    dW = []\n",
                "    dupped_dA = np.repeat(self.dA[:,np.newaxis, : ], self.n_input, axis=1)\n",
                "    for i in range(self.n_output): #convolve each output_channel through X\n",
                "      conv = self.convolve(dupped_dA[i], self.X)\n",
                "      dW.append(conv) \n",
                "    return np.array(dW)\n",
                "  def calc_dB(self):\n",
                "    return np.array(self.dA.sum(axis = 1))\n",
                "  def calc_dX(self):\n",
                "    pad_dA = self.pad_dA() #match a with x\n",
                "    flipped_W = np.flip(self.W,axis = 2).reshape(self.n_input,self.n_output, -1) # flip each filter and the in_out dim also\n",
                "    output = []\n",
                "    for i in range(self.n_input):\n",
                "      filt = flipped_W[i]\n",
                "      conv = self.convolve(filt, pad_dA)\n",
                "      output.append(conv.sum(axis = 0))\n",
                "    return np.array(output)\n",
                "  def pad_dA(self):\n",
                "    n_features_in = self.dA.shape[1]\n",
                "    n_features_out = self.X.shape[1]\n",
                "    filter_size = self.filter_size\n",
                "    padding = (n_features_out - 1 + filter_size - n_features_in) // 2\n",
                "    return self.pad(self.dA, padding)\n",
                "  def pad(self,array,padding):\n",
                "    return np.pad(array,[(0,0),(padding, padding)], mode = 'constant')\n",
                "  def convolve(self,F,X):\n",
                "    A = []\n",
                "    filter_size = F.shape[1]\n",
                "    feature_count = X.shape[1]\n",
                "    n_out_features = (feature_count - filter_size) + 1\n",
                "    for i in range(n_out_features):\n",
                "      A.append((X[:,i : i + filter_size] * F).sum(axis=1))\n",
                "    return np.array(A).T"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Forward"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[[16. 22.]\n",
                        " [17. 23.]\n",
                        " [18. 24.]]\n",
                        "correct: result:  [[16 22]\n",
                        " [17 23]\n",
                        " [18 24]]\n"
                    ]
                }
            ],
            "source": [
                "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) #shape (2, 4), (number of input channels, number of features).\n",
                "w = np.ones((3, 2, 3)) # Set to 1 for simplification of the example. (Number of output channels, number of input channels, filter size).\n",
                "b = np.array([1, 2, 3]) # (Number of output channels)\n",
                "\n",
                "conv = Conv1D(3,2,3)\n",
                "conv.B = b\n",
                "print(conv.forward(x))\n",
                "print('correct: result: ',np.array([[16, 22], [17, 23], [18, 24]]))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Let's user problem 3 as a reference to make the multichannel propagation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "X:  [[1 2 3 4]\n",
                        " [1 2 3 4]]\n",
                        "w:  [[[3. 5. 7.]\n",
                        "  [3. 5. 7.]]\n",
                        "\n",
                        " [[3. 5. 7.]\n",
                        "  [3. 5. 7.]]\n",
                        "\n",
                        " [[3. 5. 7.]\n",
                        "  [3. 5. 7.]]]\n",
                        "b:  [1. 1. 1.]\n",
                        "forward:  [[69. 99.]\n",
                        " [69. 99.]\n",
                        " [69. 99.]] correct result:  [35 50]\n",
                        "dB:  [30 30 30] correct result:  [30]\n",
                        "dW:  [[[ 50  80 110]\n",
                        "  [ 50  80 110]]\n",
                        "\n",
                        " [[ 50  80 110]\n",
                        "  [ 50  80 110]]\n",
                        "\n",
                        " [[ 50  80 110]\n",
                        "  [ 50  80 110]]] correct result:  [ 50  80 110]\n",
                        "dX:  [[ 90. 330. 510. 420.]\n",
                        " [ 90. 330. 510. 420.]] correct result:  [ 30 110 170 140]\n"
                    ]
                }
            ],
            "source": [
                "x = np.array([1,2,3,4])\n",
                "w = np.array([3, 5, 7], dtype = np.float64)\n",
                "filter_size, n_input,n_output = 3,2,3\n",
                "# print('f,nin,nout: ', filter_size, n_input,n_output)\n",
                "layer = Conv1D()\n",
                "x = np.vstack([x] * n_input)\n",
                "w = np.vstack([[np.vstack([w]*n_input)]] * n_output)\n",
                "b = np.ones(n_output)\n",
                "layer.W = w\n",
                "layer.B = b\n",
                "\n",
                "print('X: ', x)\n",
                "print('w: ', w)\n",
                "print('b: ', b)\n",
                "\n",
                "correct_forward_result = np.array([35, 50])\n",
                "a = layer.forward(x)\n",
                "print('forward: ', layer.forward(x), 'correct result: ', correct_forward_result)\n",
                "\n",
                "\n",
                "delta_a = np.vstack([[10, 20]]* n_output)\n",
                "layer.dA = delta_a\n",
                "dW = layer.calc_dW()\n",
                "dB = layer.calc_dB()\n",
                "dX = layer.calc_dX()\n",
                "\n",
                "print('dB: ', dB, 'correct result: ', correct_delta_b)\n",
                "print('dW: ', dW,'correct result: ', correct_delta_w)\n",
                "print('dX: ', dX, 'correct result: ', correct_delta_x)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Problem 5 - Implement Padding"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Note: padding added in prototype"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "x: [[1 2 3 4]\n",
                        " [1 2 3 4]]\n",
                        "padded:\n",
                        " [[0 0 0 1 2 3 4 0 0 0]\n",
                        " [0 0 0 1 2 3 4 0 0 0]]\n"
                    ]
                }
            ],
            "source": [
                "#padding example\n",
                "def add_padding(X, padding_size):\n",
                "    return np.pad(X, ((0,0),(padding_size,padding_size)), mode = 'constant', constant_values = 0)\n",
                "\n",
                "#we are conv 1d so just pad horizontally\n",
                "print(f'x: {x}')\n",
                "print(f'padded:\\n {add_padding(x, 3)}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "shapes:  (3, 2, 3) (3,) (2, 8)\n"
                    ]
                }
            ],
            "source": [
                "# test run on prototype just to see if it works\n",
                "padded_layer = Conv1D(padding = 2)\n",
                "a = padded_layer.forward(x)\n",
                "dW,dB,dX = padded_layer.backward(a)\n",
                "print('shapes: ', dW.shape,dB.shape,dX.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Problem 6 - Response to mini batch"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### NOTE\n",
                "Currently, our layer only works with one sample with many layers\n",
                "\n",
                "**Let's add many samples**\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Batch Prototype"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ConvInitializer:\n",
                "  def __init__(self, sigma = 0.1):\n",
                "    self.sigma = sigma\n",
                "  def W(self, n_output, n_input, filter_size):\n",
                "    W = self.sigma * np.random.randn(n_output, n_input, filter_size)\n",
                "    return W\n",
                "  def B(self, n_output):\n",
                "\n",
                "    B = self.sigma * np.random.randn(n_output)\n",
                "    return B"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Conv1DBatch:\n",
                "  def __init__(self, filter_size = 3, n_input = 2, n_output = 3, optimizer = SGD(), initializer = ConvInitializer(), padding = 0):\n",
                "    # Initialize\n",
                "    self.optimizer = optimizer\n",
                "    self.filter_size = filter_size\n",
                "    # Initialize self.W and self.B using the initializer method\n",
                "    self.filter_size = filter_size\n",
                "    self.n_input = n_input\n",
                "    self.n_output = n_output\n",
                "\n",
                "    if initializer == None:\n",
                "      self.W = np.ones((n_output, n_input,filter_size))\n",
                "      self.B = np.ones(n_output)\n",
                "    else:\n",
                "      self.W = initializer.W(n_output, n_input, filter_size)\n",
                "      self.B = initializer.B(n_output)\n",
                "    #padding and stride\n",
                "    self.padding = padding\n",
                "        \n",
                "  def forward(self,X): #! NOTE: X must be 3 dimensional (batch_size, channel_count, feature_count)\n",
                "    self.batch_size = len(X)\n",
                "    X = np.pad(X,[(0,0),(0,0),(self.padding, self.padding)], mode = 'constant')\n",
                "    self.X = X\n",
                "    result = []\n",
                "    for x in self.X:\n",
                "      result.append(self._forward_sample(x))\n",
                "    # print('forward: ', np.array(result).shape)\n",
                "    return np.array(result)\n",
                "  def _forward_sample(self,X):    \n",
                "    output = []\n",
                "    for i in range(self.n_output):\n",
                "      filt = self.W[i]\n",
                "      bias = self.B[i]\n",
                "      conv = self.convolve(filt, X)\n",
                "      output.append(conv.sum(axis = 0) + bias)\n",
                "    return np.array(output)\n",
                "\n",
                "  def backward(self, dA):\n",
                "    # update\n",
                "    self.dA = dA\n",
                "    self.dX = []\n",
                "\n",
                "    for i in range(self.batch_size):\n",
                "      self.dW = self.calc_dW(i)\n",
                "      self.dB = self.calc_dB(i)\n",
                "      self.dx = self.calc_dx(i)\n",
                "      self = self.optimizer.update(self)\n",
                "      self.dX.append(self.dx) #! keep updating while calculating error for prev layer\n",
                "      # print('dx: ', i, self.dx.shape,self.dx)\n",
                "    self.dX = np.array(self.dX)\n",
                "    return self.dW, self.dB , self.dX\n",
                "    \n",
                "\n",
                "  def calc_dW(self,sample_index):\n",
                "    dW = []\n",
                "    dupped_dA = np.repeat(self.dA[sample_index][:,np.newaxis, : ], self.n_input, axis=1)\n",
                "    for i in range(self.n_output): #convolve each output_channel through X\n",
                "      conv = self.convolve(dupped_dA[i], self.X[sample_index])\n",
                "      dW.append(conv) \n",
                "    return np.array(dW)\n",
                "  def calc_dB(self,sample_index):\n",
                "    return np.array(self.dA[sample_index].sum(axis = 1))\n",
                "\n",
                "  def calc_dx(self, sample_index): #! careful\n",
                "    pad_dA = self.pad_dA(sample_index) #match a with x\n",
                "    flipped_W = np.flip(self.W,axis = 2).reshape(self.n_input,self.n_output, -1) # flip each filter and the in_out dim also\n",
                "    output = []\n",
                "    for i in range(self.n_input):\n",
                "      filt = flipped_W[i]\n",
                "      conv = self.convolve(filt, pad_dA)\n",
                "      output.append(conv.sum(axis = 0))\n",
                "    return np.array(output)\n",
                "  def pad_dA(self,sample_index):\n",
                "    array = self.dA[sample_index]\n",
                "    n_features_in = array.shape[1]\n",
                "    n_features_out = self.X.shape[-1]\n",
                "    filter_size = self.filter_size\n",
                "    padding = (n_features_out - 1 + filter_size - n_features_in) // 2\n",
                "    return self.pad(array, padding)\n",
                "  def pad(self,array,padding):\n",
                "    return np.pad(array,[(0,0),(padding, padding)], mode = 'constant')\n",
                "  def convolve(self,F,X):\n",
                "    A = []\n",
                "    filter_size = F.shape[-1]\n",
                "    feature_count = X.shape[-1]\n",
                "    n_out_features = (feature_count - filter_size) + 1\n",
                "    for i in range(n_out_features):\n",
                "      A.append((X[...,i : i + filter_size] * F).sum(axis=-1))\n",
                "    return np.array(A).T\n",
                "\n",
                "# layer = Conv1DBatch(padding = 2)\n",
                "# X = np.array([[[1,2,3],[4,5,6]], [[2,3,4],[5,6,7]]])\n",
                "# layer.B = np.array([1,2,3])\n",
                "# fr = layer.forward(X)\n",
                "# print(fr.shape, fr)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(2, 2, 4) [[[1 2 3 4]\n",
                        "  [1 2 3 4]]\n",
                        "\n",
                        " [[1 2 3 4]\n",
                        "  [1 2 3 4]]]\n",
                        "[[[69. 99.]\n",
                        "  [69. 99.]\n",
                        "  [69. 99.]]\n",
                        "\n",
                        " [[69. 99.]\n",
                        "  [69. 99.]\n",
                        "  [69. 99.]]]\n",
                        "delta_a:  (2, 3, 2) [[[10 20]\n",
                        "  [10 20]\n",
                        "  [10 20]]\n",
                        "\n",
                        " [[10 20]\n",
                        "  [10 20]\n",
                        "  [10 20]]]\n",
                        "dB:  [30 30 30] correct result:  [30]\n",
                        "dW:  [[[ 50  80 110]\n",
                        "  [ 50  80 110]]\n",
                        "\n",
                        " [[ 50  80 110]\n",
                        "  [ 50  80 110]]\n",
                        "\n",
                        " [[ 50  80 110]\n",
                        "  [ 50  80 110]]] correct result:  [ 50  80 110]\n",
                        "dX:  [[[ 90. 330. 510. 420.]\n",
                        "  [ 90. 330. 510. 420.]]\n",
                        "\n",
                        " [[ 75. 276. 429. 354.]\n",
                        "  [ 75. 276. 429. 354.]]] correct result:  [ 30 110 170 140]\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "#batch test\n",
                "x = np.array([1,2,3,4])\n",
                "w = np.array([3, 5, 7], dtype = np.float64)\n",
                "filter_size, n_input,n_output = 3,2,3\n",
                "# print('f,nin,nout: ', filter_size, n_input,n_output)\n",
                "layer = Conv1DBatch(filter_size = filter_size, n_input = n_input, n_output =n_output)\n",
                "x = np.vstack([x] * n_input)\n",
                "w = np.vstack([[np.vstack([w]*n_input)]] * n_output)\n",
                "b = np.ones(n_output)\n",
                "layer.W = w\n",
                "layer.B = b\n",
                "\n",
                "#add dim\n",
                "batch_size = 2\n",
                "x = np.vstack([[x]] * batch_size)\n",
                "\n",
                "print(x.shape,x)\n",
                "fr = layer.forward(x)\n",
                "print(fr)\n",
                "\n",
                "delta_a = np.vstack([[10, 20]]* n_output)\n",
                "delta_a = np.vstack([[delta_a]] * batch_size)\n",
                "# layer.dA = delta_a\n",
                "print('delta_a: ', delta_a.shape, delta_a)\n",
                "dW,dB,dX = layer.backward(delta_a)\n",
                "\n",
                "print('dB: ', dB, 'correct result: ', correct_delta_b)\n",
                "print('dW: ', dW,'correct result: ', correct_delta_w)\n",
                "print('dX: ', dX, 'correct result: ', correct_delta_x)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Modified Copy Old Code"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.preprocessing import OneHotEncoder\n",
                "from sklearn.metrics import accuracy_score\n",
                "\n",
                "\n",
                "#! activations..............\n",
                "\n",
                "class ActivationFunction():\n",
                "  def forward(self,A):\n",
                "    pass\n",
                "  def backward(self,dZ):\n",
                "    pass\n",
                "class Tanh(ActivationFunction):\n",
                "  def forward(self,A):\n",
                "    self.A = A\n",
                "    Z = (np.exp(A) - np.exp(-A)) / (np.exp(A) + np.exp(-A))\n",
                "    return Z\n",
                "  def backward(self,dZ):\n",
                "    A = self.A\n",
                "    dA = dZ * (1 - np.tanh(A) ** 2)\n",
                "    return dA\n",
                "  \n",
                "class SoftMax(ActivationFunction):\n",
                "  def forward(self,A):\n",
                "    self.A = A\n",
                "    Z = np.exp(A) / np.sum(np.exp(A), axis = 1).reshape(-1,1)\n",
                "    return Z\n",
                "  def backward(self,Z,Y):\n",
                "    A = self.A\n",
                "    nb = Z.shape[0]\n",
                "    dA = 1/nb * (Z - Y)\n",
                "    return dA\n",
                "\n",
                "#! Full Connected Layer ...........................\n",
                "\n",
                "class FC:\n",
                "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
                "        # Initialize\n",
                "        self.optimizer = optimizer\n",
                "        self.n_nodes1, self.n_nodes2 = n_nodes1, n_nodes2\n",
                "        # Initialize self.W and self.B using the initializer method\n",
                "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
                "        self.B = initializer.B(n_nodes2)\n",
                "        pass\n",
                "    def forward(self, X):   \n",
                "        self.X = X\n",
                "        A = X @ self.W + self.B\n",
                "        return A\n",
                "    def backward(self, dA):\n",
                "        # update\n",
                "        self.dA = dA\n",
                "        self.dW = self.calc_dW()\n",
                "        self.dB = self.calc_dB()\n",
                "        self.dZ = self.calc_dZ()\n",
                "\n",
                "        self = self.optimizer.update(self)\n",
                "        return self.dW, self.dB, self.dZ\n",
                "        # return self.dZ\n",
                "    def calc_dB(self):\n",
                "        dB = self.dA.sum(axis = 0).reshape(1,-1)  \n",
                "        return dB\n",
                "    def calc_dW(self):\n",
                "        dW = self.X.T @ self.dA\n",
                "        return dW\n",
                "    def calc_dZ(self):\n",
                "        dZ = self.dA @ self.W.T\n",
                "        return dZ\n",
                "\n",
                "class AdaGrad():\n",
                "    \"\"\"\n",
                "    Stochastic gradient descent\n",
                "    Parameters\n",
                "    ----------\n",
                "    lr : Learning rate\n",
                "    \"\"\"\n",
                "    def __init__(self, lr):\n",
                "        self.lr = lr\n",
                "        self.Hw = 0\n",
                "        self.Hb = 0\n",
                "    def update(self, layer):\n",
                "        \"\"\"\n",
                "        Update weights and biases for a layer\n",
                "        Parameters\n",
                "        ----------\n",
                "        layer : Instance of the layer before update\n",
                "        \"\"\"\n",
                "        #ada weight\n",
                "        dW = layer.dW\n",
                "        dB = layer.dB\n",
                "\n",
                "        eps = 1e-6\n",
                "        self.Hw += dW ** 2\n",
                "        self.Hb += dB ** 2\n",
                "        layer.W += - self.lr * (1 / np.sqrt(self.Hw + eps)) * dW\n",
                "        layer.B += - self.lr * (1 / np.sqrt(self.Hb + eps)) * dB\n",
                "        return layer\n",
                "\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Problem 8 - Learing and Estimation"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Deep Neural Net Proto\n",
                "**Tweaked from the old implementation to fit with new Conv1D**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DeepNeuralNetworkClassifier():\n",
                "    def __init__(self,encoder, max_iter = 5, lr = 0.1, batch_size = 20 ,\n",
                "               verbose = False, debug = False):\n",
                "        self.epoch = max_iter\n",
                "        self.verbose = verbose\n",
                "        self.debug = debug\n",
                "        self.lr = lr\n",
                "        #other non-parametric vars:\n",
                "        self.encoder = encoder\n",
                "        self.sigma = 0.01\n",
                "        self.batch_size = batch_size # batch size \n",
                "        \n",
                "        #layers and activations\n",
                "        self.layers = []\n",
                "        self.activations = []\n",
                "\n",
                "    def add(self,layer, activation):\n",
                "      self.layers.append(layer)\n",
                "      self.activations.append(activation)\n",
                "        \n",
                "    def enum_layer_act(self, rev = False):\n",
                "      zipped = zip(self.layers, self.activations)\n",
                "      if rev:\n",
                "        return enumerate(reversed(list(zipped)))\n",
                "      return enumerate(zipped)\n",
                "      \n",
                "    def forward_prop(self,X):\n",
                "      Z = X\n",
                "      for i, (layer, activation) in self.enum_layer_act():\n",
                "        A = layer.forward(Z)\n",
                "        Z = activation.forward(A)\n",
                "        if self.debug:\n",
                "          print(f'Z{i+1}: ', Z.shape, A.shape)\n",
                "      return Z\n",
                "\n",
                "    def backward_prop(self,Z,y):\n",
                "      dA = self.activations[-1].backward(Z,y)\n",
                "      if self.debug:\n",
                "        print(f'Backward last Y: ', dA.shape)\n",
                "      for i, (layer, activation) in self.enum_layer_act(rev = True):\n",
                "        if i == 0: #last layer has different activation backward!\n",
                "          dW,dB,dZ = layer.backward(dA)\n",
                "          if self.debug:\n",
                "            print(f'Backward layer: {len(self.layers) - (i)} ', dZ.shape)\n",
                "          continue\n",
                "        dA = activation.backward(dZ)\n",
                "        dW,dB,dZ = layer.backward(dA)\n",
                "        if self.debug:\n",
                "          print(f'Backward layer: {len(self.layers) - (i)} ', dZ.shape)\n",
                "        \n",
                "    \n",
                "    def cross_entropy_error(self,Z,y):\n",
                "      return (np.log(Z) * y).sum() / (- len(Z))\n",
                "\n",
                "    def predict(self,X):\n",
                "      y = np.zeros(X.shape[0])\n",
                "      Z  = self.forward_prop(X)\n",
                "      return self.encoder.transform(np.argmax(Z, axis = 1).reshape(-1,1))\n",
                "\n",
                "\n",
                "    def fit(self,X,y, X_val = None, y_val = None):\n",
                "      #prepare\n",
                "      self.n_features = X.shape[1]\n",
                "      self.lenx = len(X)\n",
                "      self.batch_count = len(GetMiniBatch(X,y,batch_size= self.batch_size)) #for debug\n",
                "\n",
                "      if self.verbose:\n",
                "          print('X shape: ', X.shape, 'type: ', X.dtype)\n",
                "          print('Batch count: ', self.batch_count)\n",
                "          # for i, (layer, activation) in self.enum_layer_act():\n",
                "          #   print(f'Layer {i+1}: ', layer.n_nodes1, layer.n_nodes2)\n",
                "          #   print(f'Activ: {i+1}:', activation.__class__.__name__)\n",
                "\n",
                "      #train\n",
                "      self.loss = np.zeros(self.epoch)\n",
                "      self.accuracy = np.zeros(self.epoch)\n",
                "      for i in range(self.epoch): #one full data ilteration\n",
                "          if self.verbose: print('Epoch: ', i)\n",
                "          self.get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\n",
                "          for idx, (mini_X_train, mini_y_train) in enumerate(self.get_mini_batch):\n",
                "              if self.debug: print('Current batch: ', idx, mini_X_train.shape, mini_y_train.shape)\n",
                "              #train mini_batch\n",
                "              Z = self.forward_prop(mini_X_train)\n",
                "              self.backward_prop(Z,mini_y_train)\n",
                "\n",
                "\n",
                "          #record loss data\n",
                "          if self.debug: print('Predicting and recording loss/acc')\n",
                "          Z = self.forward_prop(X)\n",
                "          self.loss[i] = self.cross_entropy_error(Z,y)\n",
                "          train_pred = self.predict(X)\n",
                "          self.accuracy[i]  = accuracy_score(train_pred,y)\n",
                "          if self.verbose:\n",
                "              print(f'Loss {i}:', self.loss[i])\n",
                "              print(f'Acc {i}:', self.accuracy[i])\n",
                "              \n",
                "      #verbose\n",
                "      if self.verbose:\n",
                "          print('Final train loss:',self.loss[-1])\n",
                "          print('Final train accuracy:',self.accuracy[-1])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\ADMIN\\AppData\\Local\\Temp/ipykernel_728/2615912318.py:8: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
                        "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
                        "  X_train = X_train.astype(np.float)\n",
                        "C:\\Users\\ADMIN\\AppData\\Local\\Temp/ipykernel_728/2615912318.py:9: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
                        "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
                        "  X_test = X_test.astype(np.float)\n"
                    ]
                }
            ],
            "source": [
                "#data set\n",
                "from keras.datasets import mnist\n",
                "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
                "# #reshape\n",
                "# X_train = X_train.reshape(-1, 784)\n",
                "# X_test = X_test.reshape(-1, 784)\n",
                "#scaling\n",
                "X_train = X_train.astype(np.float)\n",
                "X_test = X_test.astype(np.float)\n",
                "X_train /= 255\n",
                "X_test /= 255\n",
                "#one hot encode for multiclass labels!\n",
                "from sklearn.preprocessing import OneHotEncoder\n",
                "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
                "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
                "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
                "#validation split\n",
                "from sklearn.model_selection import train_test_split\n",
                "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, train_size=0.5)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(30000, 28, 28)\n"
                    ]
                }
            ],
            "source": [
                "print(X_train.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": [
                "#flatten layer\n",
                "class FlattenLayer():\n",
                "    def __init__(self):\n",
                "        self.in_shape = None\n",
                "    def forward(self,X):\n",
                "        if (X.shape != self.in_shape): print('self: ', self.in_shape, 'x: ',X.shape)\n",
                "        self.in_shape = X.shape\n",
                "        return X.reshape(-1,X.shape[-1]) #flatten the channel to 1\n",
                "    def backward(self,dA):\n",
                "        return None, None ,dA.reshape(self.in_shape)\n",
                "class TransparentFunction():\n",
                "    def forward(self,X):\n",
                "        return X\n",
                "    def backward(self,dA):\n",
                "        return dA"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "train shape:  (30000, 28, 28)\n",
                        "input features:  28\n",
                        "input channels:  20\n"
                    ]
                }
            ],
            "source": [
                "# test run deep neural\n",
                "model = DeepNeuralNetworkClassifier(enc, debug = False, verbose = True, max_iter = 5)\n",
                "n_in_features = X_train.shape[1]\n",
                "batch_size  = 20\n",
                "print('train shape: ', X_train.shape)\n",
                "print('input features: ', n_in_features)\n",
                "print('input channels: ', batch_size)\n",
                "\n",
                "\n",
                "in_channel = X_train.shape[1]\n",
                "\n",
                "l1 = Conv1DBatch(filter_size = 3, n_input = in_channel, n_output = 1, optimizer = AdaGrad(0.1))\n",
                "model.add(l1,Tanh())\n",
                "lshape = FlattenLayer()\n",
                "model.add(lshape,TransparentFunction())\n",
                "l2 = FC(26,100, SimpleInitializer(),SGD()) #cause output of conv is 394\n",
                "model.add(l2,Tanh())\n",
                "l3 = FC(100,10, SimpleInitializer(),SGD())\n",
                "model.add(l3,SoftMax())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "xshape:  (30000, 28, 28) yshape:  (30000, 10)\n",
                        "X shape:  (30000, 28, 28) type:  float64\n",
                        "Batch count:  1500\n",
                        "Epoch:  0\n",
                        "self:  None x:  (20, 1, 26)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\ADMIN\\AppData\\Local\\Temp/ipykernel_728/3471495081.py:53: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
                        "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
                        "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "self:  (20, 1, 26) x:  (30000, 1, 26)\n",
                        "Loss 0: 1.1809910515214375\n",
                        "Acc 0: 0.6013333333333334\n",
                        "Epoch:  1\n",
                        "self:  (30000, 1, 26) x:  (20, 1, 26)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\ADMIN\\AppData\\Local\\Temp/ipykernel_728/3471495081.py:53: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
                        "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
                        "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "self:  (20, 1, 26) x:  (30000, 1, 26)\n",
                        "Loss 1: 1.0935130522746976\n",
                        "Acc 1: 0.6294666666666666\n",
                        "Epoch:  2\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\ADMIN\\AppData\\Local\\Temp/ipykernel_728/3471495081.py:53: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
                        "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
                        "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "self:  (30000, 1, 26) x:  (20, 1, 26)\n",
                        "self:  (20, 1, 26) x:  (30000, 1, 26)\n",
                        "Loss 2: 1.0396127040649055\n",
                        "Acc 2: 0.6495333333333333\n",
                        "Epoch:  3\n",
                        "self:  (30000, 1, 26) x:  (20, 1, 26)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\ADMIN\\AppData\\Local\\Temp/ipykernel_728/3471495081.py:53: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
                        "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
                        "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "self:  (20, 1, 26) x:  (30000, 1, 26)\n",
                        "Loss 3: 1.0019688489200658\n",
                        "Acc 3: 0.6639333333333334\n",
                        "Epoch:  4\n",
                        "self:  (30000, 1, 26) x:  (20, 1, 26)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\ADMIN\\AppData\\Local\\Temp/ipykernel_728/3471495081.py:53: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
                        "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
                        "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "self:  (20, 1, 26) x:  (30000, 1, 26)\n",
                        "Loss 4: 0.9736878055673618\n",
                        "Acc 4: 0.6751\n",
                        "Final train loss: 0.9736878055673618\n",
                        "Final train accuracy: 0.6751\n"
                    ]
                }
            ],
            "source": [
                "print('xshape: ', X_train.shape, 'yshape: ', y_train.shape)\n",
                "model.fit(X_train, y_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 432x288 with 1 Axes>"
                        ]
                    },
                    "metadata": {
                        "needs_background": "light"
                    },
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# loss result\n",
                "plt.plot(model.loss, label = 'TrainLoss')\n",
                "plt.plot(model.accuracy, label = 'TrainAccuracy')\n",
                "plt.title('Training Metrics')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "-----------------------------------------------------------------------------------------"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This is the end of my assignment. Thank you for reading!"
            ]
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "a9d1005c97ab2ad518cdcb8630936f46560d68ad33a376b7934a63663128f392"
        },
        "kernelspec": {
            "display_name": "Python 3.8.10 64-bit (windows store)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
