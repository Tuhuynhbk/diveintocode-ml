{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Name: Huynh Truong Tu\n",
    " Below is my assignment for Sprint16's \"Thesis Intro\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Read and summarize 5 paper in interested field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chosen field -  **Natural language processing**\n",
    "Focus on current trend and what is being done in this field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Papers\n",
    "1. SentiHood: Targeted Aspect Based Sentiment Analysis Dataset for Urban\n",
    "Neighbourhoods [Link](https://arxiv.org/pdf/1610.03771.pdf)\n",
    "2. Data-driven Summarization of Scientific Articles [Link](https://arxiv.org/pdf/1804.08875.pdf)\n",
    "3. Make Lead Bias in Your Favor -A Simple and Effective Method for News Summarization [Link](https://openreview.net/attachment?id=ryxAY34YwB&name=original_pdf)\n",
    "4. Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence [Link](https://arxiv.org/pdf/1903.09588.pdf)\n",
    "5. Implicit and Explicit Aspect Extraction in Financial Microblogs [Link](https://aclanthology.org/W18-3108.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 1 - SentiHood: Targeted Aspect Based Sentiment Analysis Dataset for Urban Neighbourhoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper Reason\n",
    "This paper introduces the task of targeted aspect-based sentiment analysis (TABSA). This work extends aspect-based sentiment analysis that assumes only a single entity per document and targeted sentiment analysis that assumes only a single sentiment towards a target entity. The contributions are as follow:\n",
    "\n",
    "## Object of this paper\n",
    "- Introduce the task of targeted aspect-based sentiment analysis (TABSA)\n",
    "- Introduce the SentiHood dataset, which it’s base don text taken from question answering platform of Yahoo!\n",
    "- Develop few strong baseline models using logistic regression and LSTM for future benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief\n",
    "\n",
    "### The limitation of aspect-based sentiment analysis (ABSA) and targeted sentiment analysis\n",
    "Sentiment analysis (overall sentiment) -> ABSA and targeted sentiment analysis -> Targeted ABSA.\n",
    "\n",
    "ABSA involves extracting sentiment towards different aspects of an entity in the same unit of text. The dataset used for ABSA mostly assume that only one entity is discussed in one review snippet but the opinion on multiple aspects can be expressed. Targeted sentiment analysis perform sentiment analysis towards a certain target entity mentions in given sentences. This task assumes only a single sentiment for each entity and dataset so far only contains a single target entity per unit of text.\n",
    "\n",
    "### SentiHood Dataset\n",
    "- Contains annotated sentences with one or two location entity mentions\n",
    "- It has 5215 sentences, of which 3862 sentence contains a single location entity and 1353 contains two location entities\n",
    "- The dataset only has positive or negative sentiment:\n",
    "- Location entity names are masked by location1 and location2\n",
    "- The dataset also include polarity class “None” where a sentence does not contain an opinion for the aspect a of location l.\n",
    "\n",
    "### Experimental idea\n",
    "Selected the four most frequent aspects from dataset: “price”, “safety”, “transit-location”, and “general”. Results are broken down to single location sentences, two locations sentences, and overall test set. The idea is that results on single location sentences will showcase the model’s ability to perform correct sentiment analysis and the results on two locations sentences will show the model’s ability to detect the relevant sentiment of an aspect and recognise the target entity of the opinion.\n",
    "\n",
    "### Baseline Models\n",
    "- Different variation of logistic regression with linguistic features\n",
    "- LSTM with final and location output state\n",
    "\n",
    "### Evaluation \n",
    "- F1 score — aspect detection\n",
    "- Accuracy — sentiment classification\n",
    "- AUC (Area under the ROC curve) — both aspect and sentiment detection\n",
    "### Result\n",
    "- The best performing model is logistic regression with location masking and POS information. The underperforming of LSTM might be due to lack of training data.\n",
    "- The consistent outperformance of logistic regression over LSTM highlights the advantage of features engineering when the volume of data is low.\n",
    "\n",
    "### Conclusion and Future Work\n",
    "- Ways to improve baselines can involve using parse trees for identifying the context of each location\n",
    "- Data augmentation can be used to make models more robust to variations in the data\n",
    "\n",
    "### Personal Ref \n",
    "[Link](https://towardsdatascience.com/day-104-of-nlp365-nlp-papers-summary-sentihood-targeted-aspect-based-sentiment-analysis-f24a2ec1ca32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 2 - Data-driven Summarization of Scientific Articles "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper Reason\n",
    "Created two multi-sentence summarisation datasets from scientific articles: the title-abstract pairs (title-gen) and abstract-body pairs (abstract-gen) and applied a wide range of extractive and abstractive models to it. The title-gen dataset consists of 5 million biomedical papers whereas the abstract-gen dataset consists of 900K papers. The analysis show that scientific papers are suitable for data-driven summarisation.\n",
    "## What is data-driven summarisation?\n",
    "- It is a way of saying the recent SOTA results of summarisation models rely heavily on large volume of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief\n",
    "### Datasets\n",
    "The two evaluation datasets are title-gen and abstract-gen. Title-gen was constructed using MEDLINE and abstract-gen was conducted using PubMed. The title-gen pairs the abstract to the title of the paper whereas the abstract-gen dataset pairs the full body (without tables and figures) to the abstract summary. The text processing pipeline is as follows:\n",
    "1. Tokenisation and lowercase\n",
    "2. Removal of URLs\n",
    "3. Numbers are replaced by # token\n",
    "4. Only include pairs with abstract length 150–370 tokens, title length 6–25 tokens and body length 700–10000 tokens\n",
    "\n",
    "### Experimental Setup and Results\n",
    "**Models Comparison**\n",
    "- Extractive summarisation methods. Two unsupervised baselines here: TFIDF-emb and rwmd-rank. TFIDF-emb creates sentence representation by computing a weighted sum of its constituent word embeddings. Rwmd-rank ranks sentences by how similar the sentence is compared to all the other sentences in the document. Rwmd stands for Relaxed Word Mover’s Distance, which it’s the formula used to compute similarity and subsequently LexRank is used to rank the sentences.\n",
    "- Abstractive summarisation methods. Three baselines here: lstm, fconv, and c2c. Lstm is the common LSTM encoder-decoder model but with an attention mechanism at the word-level. Fconv is a CNN encoder-decoder on subword-level, separating words into smaller units using byte-pair encoding (BPE). Character-level models are good at dealing with rare / out-of-vocabulary (OOV) words. C2c is a character-level encoder-decoder model. It builds character representations from the input using CNN and feed it into an LSTM encoder-decoder model.\n",
    "\n",
    "### Results\n",
    "1. Large variation of sentence locations selected by extractive models on title-gen, with first sentence in the abstract being the most important\n",
    "2. Many abstractive generated titles tend to be of high quality, demonstrating their ability to select important information\n",
    "3. Lstm tends to generate more novel words whereas c2c and fconv tend to copy more from input text\n",
    "4. The generated titles occasionally make mistakes by using incorrect words, being too generic and fail to capture the main point of the paper. This could all lead to factual inconsistencies\n",
    "5. For abstract-gen, it appears that introduction and conclusion sections are most relevant for generating abstract. However, important content are spread across sections and sometimes the reader focuses more about the methodology and results\n",
    "6. Output of fconv abstractive model is of bad quality where it lacks coherent and content flow. There is also the common problem of repeated sentence or phrases in the summary\n",
    "\n",
    "### Conclusion and Future Work\n",
    "There was a mixed results where the models performed well in title generation but struggled with abstract generation. This can be explained by the high-level of difficulty in understanding long input and output sequences. A future work is a hybrid extractive-abstractive end-to-end approaches.\n",
    "\n",
    "### Personal ref\n",
    "[link](https://towardsdatascience.com/day-116-of-nlp365-nlp-papers-summary-data-driven-summarization-of-scientific-articles-3fba016c733b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 3 - Make Lead Bias in Your Favor - A Simple and Effective Method for News Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective and Contribution\n",
    "The objective is to use existing lead bias in news data to pretrain summarisation models on unlabelled datasets. We want the model to predict lead sentences using the rest of the article. Lead bias is a common problem in news dataset, where few sentences at the beginning of the article contains the most important information and so models trained on news dataset has a bias towards selecting those sentences and ignore sentences later on in the article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief \n",
    "\n",
    "### Datasets\n",
    "Collected 21.4M articles (June 2016 — June 2019) after filtering articles based on the overlapping non-stopping words ratio between the top 3 sentences and the rest of the article. A high overlapping non-stopping words ratio tells us that there is a strong semantic connection.\n",
    "\n",
    "Evaluation is made on three benchmark news summarisation datasets:\n",
    "\n",
    "1. New York Times (NYT) corpus — 104K news articles\n",
    "2. Xsum — 227K news articles\n",
    "3. CNN/Daily Mail — 312K news articles\n",
    "\n",
    "### Methodology\n",
    "![image info](./pic1.png)\n",
    "\n",
    "Given a news article, we take the lead-3 as the target summary and use the rest of the article as the news content as shown in the figure above. This allows us to utilise unlabelled news datasets to train our summarisation models. This pretraining method can be apply to any datasets with structural bias, for example, academic papers with abstracts or books with tables of contents. However, the pretraining needs careful examine and cleaning to ensure we have a good target summary for our content.\n",
    "\n",
    "### Models Comparison\n",
    "- Lead-X: uses the top X sentences as summary (X = 3 for NYT and CNN/DM and X = 1 for XSum)\n",
    "- PTGen: pointer-generator network\n",
    "- DRM: uses deep reinforcement learning\n",
    "- TConvS2S: convolutional neural network\n",
    "- BottomUp: two-step approach for summarisation\n",
    "- SEQ: uses reconstruction and topic loss\n",
    "- GPT-2: pretrained language model\n",
    "\n",
    "### Result\n",
    "- PL-FT model outperformed all baseline models on both NYT and Xsum dataset. On CNN/Daily Mail, it outperformed all except BottomUp.\n",
    "- PL-NoFT outperformed all the unsupervised models on CNN/Daily Mail with a significant margins. It also performed well in Xsum. PL-NoFT is the same model across all three datasets, showcasing its generalisation ability\n",
    "\n",
    "### Conclusion and Future Work\n",
    "The paper uses the lead bias existed in news data as the target summary and pretrain summarisation models. Our pretrained model without finetuning achieve SOTA results over different news summarisation datasets. Performance improved further with finetuning. Overall, this pretraining method can be applied to any datasets where there is structural bias.\n",
    "\n",
    "### Personal Ref\n",
    "[link](https://towardsdatascience.com/day-107-of-nlp365-nlp-papers-summary-make-lead-bias-in-your-favor-a-simple-and-effective-4c52b1a569b8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 4 - Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective and Contribution\n",
    "Fine-tune pretrained BERT for Targeted Aspect-Based Sentiment Analysis (TABSA). TABSA is the task whereby you identify fine-grained opinion polarity towards a specific aspect associated with a given target. In Aspect-Based Sentiment Analysis (ABSA), you don’t have target-aspect pairs, just aspects. The contributions of this paper:\n",
    "1. A new method of tackling TABSA by treating it as a sentence-pair classification task through creating auxiliary sentences\n",
    "2. Achieve SOTA results on SentiHood and SemEval-2014 Task 4 dataset by using fine-tuned BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "### SentiHood\n",
    "- 5215 sentences, of which 3862 contain a single target and the remainder multiple targets\n",
    "- Each sentence contains a list of target-aspect pairs with the sentiment polarity\n",
    "- Given a sentence and the target, we need to a) detect mention of an aspect for the target and b) determine the polarity of the detected target-aspect pairs\n",
    "### SemEval-2014 Task 4\n",
    "- This is ABSA and not TABSA and so they don’t have target-aspect pairs, just aspect. Allows the model to tackle subtask 3 (Aspect Detection) and subtask 4 (Aspect Polarity) at the same time.\n",
    "\n",
    "## Experiments and Results\n",
    "There are two experiment settings as we have two different datasets here.\n",
    "### SentiHood Experiment Setup\n",
    "The evaluation of SentiHood only consider the 4 most frequently seen aspects in the dataset (general, price, transit-location, safety). The following are the chosen models for comparisons:\n",
    "- **Logistic Regression (LR).** LR with n-gram and pos-tag features\n",
    "- **LSTM-Final.** biLSTM with final state as representation\n",
    "- **LSTM-Loc.** biLSTM with the state associated with target position as representaion\n",
    "- **LSTM+TA+SA.** biLSTM with complex target-level and sentence-level attention mechanism\n",
    "- **SenticLSTM.** Upgraded version of LSTM+TA+SA, introducing external information from Sentic Net\n",
    "- **Dmu-Entnet.** Bi-directional EntNet with external memory chains with a delayed memory update mechanism to track entities\n",
    "### SemEval-2014 Task 4 Experiment Setup\n",
    "BERT-pair models are compared against the best performing systems, namely, XRCE, NRC-Canada, and ATAE-LSTM.\n",
    "\n",
    "## Result\n",
    "### SentiHood Results\n",
    "- BERT-single outperformed Dmu-Entnet in aspect detection but scored a lower accuracy in sentiment classification accuracy\n",
    "- BERT-pair outperformed other models on aspect detection and sentiment classification by a substantial margin. BERT-pair-QA models tend to perform better on sentiment analysis whereas BERT-pair-NLI models tend to perform better on aspect detection\n",
    "### SemEval-2014 Task 4 Results\n",
    "- BERT-single model was enough to achieved better results on the two subtasks\n",
    "- BERT-pair models achieved a further improvements over BERT-single model\n",
    "\n",
    "## Conclusion and Future Work\n",
    "- The conversion of target-aspect pairs to auxiliary sentences is equivalent to expanding the corpus, therefore, more data for our models\n",
    "- BERT seems to perform well in QA and NLI tasks indicating an advantage in dealing with sentence pair classification. This might be attributed to unsupervised masked language model and the next sentence prediction tasks\n",
    "- Directly fine-tuning the pretrained BERT on TABSA does not yield good results. By transforming TABSA into a sentence pair classification task, the context is now similar to QA and NLI and so the advantage of pretrained BERT can be utilised\n",
    "\n",
    "The conversion from single sentence classification to sentence pair classification task has yielded strong results. Future work could apply this conversion method to other similar tasks.\n",
    "## Personal Ref\n",
    "https://towardsdatascience.com/day-103-nlp-research-papers-utilizing-bert-for-aspect-based-sentiment-analysis-via-constructing-38ab3e1630a3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 5 - Implicit and Explicit Aspect Extraction in Financial Microblogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective and Contribution\n",
    "Use unsupervised and supervised techniques to perform aspect extraction from financial microblogs. The contribution is that it is extracting a domain-specific (finance) aspects and tackling both implicit and explicit aspect extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation and Dataset\n",
    "1. Predefined a stock-investment taxonomy to extract both implicit and explicit aspects\n",
    "2. Created a corpus with 7 aspect classes and 32 aspect subclasses. The corpus has 368 messages, of which 218 are implicit aspects and 150 are explicit aspects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "1. Distributional Semantics Model (DSM)\n",
    "- Extracting candidates. Uses morpho-syntactic patterns to select relevant Noun and Verb phrases, including modifiers such as adverbs, adjectives etc… For example, “declining revenues”.\n",
    "- Computing relatedness with the classes. Once the candidates have been extracted, computing semantic relatedness involves comparing the candidates vectors with the aspect subclasses vectors. Multi-word candidates are combined into a single vector. Cosine similarity is computed for all possible pairwise combinations of tokens in each message and the highest score pair is retained.\n",
    "2. Supervised ML Models — XGBoost, Random Forests, SVM, and Conditional Random Fields\n",
    "- Feature engineering. This includes BoW (binary count, frequency count, and TFIDF), POS tagging, numerical, and predicted sentiment of entity\n",
    "- ML algorithm optimisation. 4 ML algorithms were chosen: XGBoost, Random Forests, SVM, and Conditional Random Fields. Hyperparameters were tuned using Particle Swarm Optimisation (PSO) method\n",
    "- Model selection and evaluation. The best model among DSM and ML models were chosen (using CV) and the selected model is validated using the leave-one-out option\n",
    "\n",
    "### Results\n",
    "XGBoost scored the highest accuracy and was selected. Figure below shows the results of XGBoost on aspect classes and aspect subclasses classification as well as implicit and explicit aspect classification. XGBoost scored 71% accuracy on 7-aspect classes classification, 82% on explicit aspect classification and 35% on implicit aspect classification.\n",
    "\n",
    "### Conclusion\n",
    "Explicit aspect classification performed well but implicit aspect classification still needs more work and can be tackle with larger dataset and better feature engineering.\n",
    "\n",
    "### Personal Ref\n",
    "[link](https://towardsdatascience.com/day-102-of-nlp365-nlp-papers-summary-implicit-and-explicit-aspect-extraction-in-financial-bdf00a66db41)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of my assignment. Thank you for reading!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
